{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning vs. March Madness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports gambling is one of the fastest growing industries in the country, with states continuing to pass betting-friendly legislation and companies like DraftKings, FanDuel and BetMGM experiencing increased, year-over-year revenue. One of the premier sports betting events of the year is the NCAA college basketball postseason tournament, commonly known as March Madness.\n",
    "\n",
    "We target college basketball because of our domain knowledge and the talent disparity within the sport. In professional sports, the talent gap between the best and worst teams is very small. It's unusual to see NBA spreads greater than 10 points. In college basketball, this happens regularly. Teams are 25-point underdogs on a given night and that creates betting opportunity. There also tends to be more regular competition in college basketball. With only around 30 games in a season, teams don't have the luxury to take nights off like we see in an 82-game NBA season. And with constant effort comes more predictable outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Objective\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project posits that we run a sports gambling company. We offer our customers advisory services during the busiest time of the sports year on events with the greatest amount of betting action.\n",
    "\n",
    "To maximize returns, we run a series of machine learning algorithms to model predictions for single games in the NCAA tournament. We pay particular focus to accurately predicting underdog team wins, as doing so yields higher payouts. Accuracy, and more specifically predictive accuracy, is paramount in selecting our models, as we strive to minimize risk for our customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses datasets from Kaggle's *[March Machine Learning Mania 2021](https://www.kaggle.com/c/ncaam-march-mania-2021/data)* competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the win/loss outcome for the favored team as the binary target variable, with 1 equaling a win for the favored team and 0 equaling a win for the underdog. Rankings are assigned using the reputable KenPom ratings.\n",
    "\n",
    "We then use an iterative approach to build 6 predictive, classification models: Logistic Regression, K-Nearest Neighbors, Decision Tree, Random Forest, Bagging classifier and XGBoost. We utilize hyperparameter tuning, cross-validation and scoring to select the highest performing, predictive models. This approach is applied to regular season, postseason and cumulative postseason data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files we downloaded from kaggle include in game stats for all seasons from 2003 up to 2020. Our predcitions are going to be based off the season average stats of the two relevant teams in each game. In order to get this we have to calculate the averages from all the games they played up to that point in the season, and make a new dataframe for the specific game with averages as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to make a target variable. The difficulty with this dataset is, we would like to predict whether the favorite team (based on Ken Pom rankings) won the game for betting purposes, and this dataset does not include a team raking. We had to use another dataset from kaggle which includes the team rankings, check if the winning team in the game was the favorite, and then make a binary column of Favorite Win. \n",
    "\n",
    "Ken Pomeroy, the creator of KenPom, uses analytical measures to rank each team in college basketball. He is one of the most popular and well-known names in basketball analytics and has been at the forefront of the progression mainstream acceptance of advanced stats.\n",
    "\n",
    "Once we have a target column, we can create a dataframe with full seasons worth of games with a target column, and train our model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a regular season dataframe, a postseason dataframe, a rankings dataframe, \n",
    "# and a dataframe with Ken Pom ranking from the rankings dataframe. \n",
    "\n",
    "regularseason = pd.read_csv('MDataFiles_Stage1/MRegularSeasonDetailedResults.csv')\n",
    "postseason = pd.read_csv('MDataFiles_Stage1/MNCAATourneyDetailedResults.csv')\n",
    "rankings = pd.read_csv('MDataFiles_Stage1/MMasseyOrdinals.csv')\n",
    "kenpom = rankings[(rankings['SystemName'] == 'POM') & (rankings['RankingDayNum'] >= 128)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have some more advanced stats to use for prediction. We create them here with a few calculations from the regular stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rate_cols(df):\n",
    "    # Calculates the number of possesions for each team in each game. The number of possessions\n",
    "    # has a large effect on teams averages and counting stats. Knowing this tells us how\n",
    "    # efficient a team is different factors of a game.\n",
    "    df['possessions'] = .5 * (df['FGA'] + (.475 * df['FTA']) - df['OR'] + df['TO'])\n",
    "    \n",
    "    # Effective Field Goal Percentage (eFG) gives extra value to 3-point shots. Each 3-pointer made\n",
    "    # is given the weight of 1.5 2-pointers made.\n",
    "    df['eFG'] = (df['FGM'] + (.5 * df['FGM3'])) / df['FGA']\n",
    "    \n",
    "    # True Shooting Percentage (TS%) gives weight to the 3-point shot as well as free throws made.\n",
    "    # This stat takes in account all types of scoring to determine a players shooting percentage.\n",
    "    df['TS%'] = df['Score'] / (2 * (df['FGA'] + .475 * df['FTA']))\n",
    "    \n",
    "    # Free Throw Rate (FTr) is a simple ratio of Free Throws to Field Goals Attempted. It identifies\n",
    "    # a team's abiilty to draw fouls and get to the free throw line where expected value is high.\n",
    "    df['FTr'] = df['FTA'] / df['FGA']\n",
    "    \n",
    "    # Three Point Attempt Rate (3PAr) shows how often a player shoots 3-pointers compared to other\n",
    "    # types of shots. It identifies players who attack the basket and draw fouls and players who are\n",
    "    # likely to stand outside and shoot.\n",
    "    df['3PAr'] = df['FGA3'] / (df['FGA'] + (.475*df['FTA']))\n",
    "    \n",
    "    # Offensive Rebound Percentage (OR%) is the number of offensive rebounds a team gets compared\n",
    "    # to the total number of rebounds available while they were on offense.\n",
    "    df['OR%'] = df['OR'] / (df['OR'] + df['Opp_DR'])\n",
    "    \n",
    "    # Defensive Rebound Percentage (DR%) is the number of defensive rebounds a team gets compared\n",
    "    # to the total number of rebounds available while they were on defense.\n",
    "    df['DR%'] = df['DR'] / (df['DR'] + df['Opp_OR'])\n",
    "    \n",
    "    # Rebound Percentage (REB%) is the percentage of rebounds from all available rebounds a team\n",
    "    # gets in a game.\n",
    "    df['REB%'] = (df['OR'] + df['DR']) / (df['OR'] + df['Opp_DR'] + df['Opp_OR'] + df['DR'])\n",
    "    \n",
    "    # Total Rebounds (TR) the number of rebounds a team gets in a game.\n",
    "    df['TR'] = (df['OR'] + df['DR'])\n",
    "    \n",
    "    # Assist to Turnover Ratio (ATOr) shows how well a team takes care of the ball. The higher the\n",
    "    # the number the better a team is at passing the ball and playing together.\n",
    "    df['ATOr'] = df['Ast'] / df['TO']\n",
    "    \n",
    "    # Assist Percentage (Ast%) is the number of assists a team has compared to baskets scored. The\n",
    "    # higher this percentage the better the team is at playing together and creating baskets for\n",
    "    # one another.\n",
    "    df['Ast%'] = df['Ast'] / df['FGM']\n",
    "    \n",
    "    # Steal Percentage (Stl%) is the rate at which a team gets steals during other teams possessions.\n",
    "    df['Stl%'] = df['Stl'] / (df['Opp_FGA'] + .475*df['Opp_FTA'] + df['Opp_TO'])\n",
    "    \n",
    "    # Block Percentage (Blk%) is the rate at which a team blocks shots during other teams possessions.\n",
    "    df['Blk%'] = df['Blk'] / (df['Opp_FGA'])\n",
    "    \n",
    "    # Turnover Percentage (TO_r) is the rate at which a team turnovers the ball over during their own\n",
    "    # possessions.\n",
    "    df['TO_r'] = df['TO'] / (df['FGA'] + .475*df['FTA'] + df['TO'])\n",
    "    \n",
    "    # Opponent Effective Field Goal Percentage (Opp_eFG) is a defensive measure that shows the Effective\n",
    "    # Field Goal Percentage of a team's opponents.\n",
    "    df['Opp_eFG'] = (df['Opp_FGM'] + (.5 * df['Opp_FGM3'])) / df['Opp_FGA']\n",
    "    \n",
    "    # Opponent True Shooting Percentage (Opp_TSpct) is a defensive measure that shows the True Shooting\n",
    "    # Percentage of a team's opponents.\n",
    "    df['Opp_TSpct'] = df['Opp_Score'] / (2 * (df['Opp_FGA'] + .475 * df['Opp_FTA']))\n",
    "    \n",
    "    # Opponent Free Throw Rate (Opp_FTr) is a defensive measure that shows how often a team fouls their\n",
    "    # opponents and allows them to shoot free throws.\n",
    "    df['Opp_FTr'] = df['Opp_FTA'] / df['Opp_FGA']\n",
    "    \n",
    "    # Opponent Three Point Rate (Opp_3Pr) is a defensive rate measure of how many 3-pointers an\n",
    "    # opponent makes per basket scored.\n",
    "    df['Opp_3Pr'] = df['Opp_FGA3'] / (df['Opp_FGA'] + (.475*df['Opp_FTA']))\n",
    "    \n",
    "    # Opponent Total Rebounds (Opp_TR) is the number of rebounds a team's opponent gets in a game.\n",
    "    df['Opp_TR'] = (df['Opp_OR'] + df['Opp_DR'])\n",
    "    \n",
    "    # Opponent Assist to Turnover Ratio (Opp_ATOr) is a defensive rate measure of an opponents\n",
    "    # assists per turnover. This is a good measure of defensive aggressiveness and the ability to make\n",
    "    # opponents work hard for their shots.\n",
    "    df['Opp_ATOr'] = df['Opp_Ast'] / df['Opp_TO']\n",
    "    \n",
    "    # Opponent Assist Percentage (Opp_Ast_%) is the percentage of opponent's baskets that are assisted.\n",
    "    df['Opp_Ast_%'] = df['Opp_Ast'] / df['Opp_FGM']\n",
    "    \n",
    "    # Opponent Steal Percentage (Opp_Stl_%) is the percentage possesions that end with the opponent getting\n",
    "    # a steal.\n",
    "    df['Opp_Stl_%'] = df['Opp_Stl'] / (df['FGA'] + .475*df['FTA'] + df['TO'])\n",
    "    \n",
    "    # Opponent Block Percentage (Opp_Blk_%) is the percentage of opponent's possessions that end in a\n",
    "    # block.\n",
    "    df['Opp_Blk_%'] = df['Opp_Blk'] / (df['FGA'])\n",
    "    \n",
    "    # Opponent Turnover Rate (Opp_TO_r) is the percentage of opponent's possesions that end in a \n",
    "    # turnover.\n",
    "    df['Opp_TO_r'] = df['Opp_TO'] / (df['Opp_FGA'] + .475*df['Opp_FTA'] + df['Opp_TO'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take all the in-game stats of a teams season, and put it into one dataframe. Then (if the paramter average is set to True), put that into a single row with the teams averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the stats for a team in the regular season. The parameters are:\n",
    "# the team id number, the season, the day number of the season, and whether you want to see\n",
    "# each individual game or the season averages.\n",
    "\n",
    "def team_regular_season(team_id,Season = all, DayNum = all, average = True):\n",
    "    \n",
    "    # create a dataframe that has all the wins/losses of a team during the season up to the given day number.\n",
    "    teamwins = regularseason[(regularseason['WTeamID'] == team_id) & (regularseason['Season'] == Season) & (regularseason['DayNum'] < DayNum)] \n",
    "    teamlosses = regularseason[(regularseason['LTeamID'] == team_id) & (regularseason['Season'] == Season) & (regularseason['DayNum'] < DayNum)]\n",
    "    \n",
    "    # split the dataframes to only have the offensive statistics of the given team\n",
    "    teamwinsoffense = teamwins.filter(like = 'W')    \n",
    "    teamlossesoffense = teamlosses.filter(like = 'L')\n",
    "\n",
    "    # split the dataframes to only have the defensive statistics of the given team\n",
    "    teamwinsoffense.columns = teamwinsoffense.columns.str.lstrip('W')\n",
    "    teamlossesoffense.columns = teamlossesoffense.columns.str.lstrip('L')\n",
    "\n",
    "    # concat the two offensive dataframes\n",
    "    teamoffense = pd.concat([teamwinsoffense, teamlossesoffense])\n",
    "\n",
    "    # create oppposing team statistics from the given games\n",
    "    teamwinsopponents = teamwins.filter(like = 'L')\n",
    "    teamlossesopponents = teamlosses.filter(like = 'W')\n",
    "\n",
    "    # rename the columns from the opponent statistics dataframes\n",
    "    teamwinsopponents.columns = teamwinsopponents.columns.str.lstrip('L')\n",
    "    teamlossesopponents.columns = teamlossesopponents.columns.str.lstrip('W')\n",
    "\n",
    "    # concat the dataframes from opponent wins and losses\n",
    "    teamopponents = pd.concat([teamwinsopponents, teamlossesopponents])\n",
    "    \n",
    "    # concat the team stats and the opponent stats to create a full picture of the team's season\n",
    "    teamtotal = pd.concat([teamoffense, teamopponents.add_prefix('Opp_')], axis = 1)\n",
    "    \n",
    "    teamtotal.drop('Opp_TeamID', axis = 1, inplace = True)\n",
    "    \n",
    "    # return either the season averages or the game by game statistics\n",
    "    if average == True:\n",
    "        teamtotal = teamtotal.groupby('TeamID').mean().reset_index()\n",
    "        add_rate_cols(teamtotal)\n",
    "    \n",
    "    if average == False:\n",
    "        add_rate_cols(teamtotal)\n",
    "        \n",
    "    return teamtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 0.056832 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here's an example of a team_regular_season output.\n",
    "start = time.time()\n",
    "marquette = team_regular_season(1266, 2020, 132, True)\n",
    "end = time.time()\n",
    "print('Execution time = %.6f seconds' % (end-start))\n",
    "marquette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the end of year rankings of a team for a specific year with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "Name: OrdinalRank, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_KenPom(team_id, Year):\n",
    "    rank = kenpom[(kenpom['TeamID'] == team_id) & (kenpom['Season'] == Year)]\n",
    "    rank.reset_index(inplace = True)\n",
    "    return rank.OrdinalRank\n",
    "\n",
    "get_KenPom(1181, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take in a specified matchup and output a game with the two teams season stats and whether the favorite team one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_game(team1_id, team2_id, Year = 2020, DayNum = 132):\n",
    "    \n",
    "    # from the Team IDs get a dataframe with the average seasonlong team statistics.\n",
    "    team1 = team_regular_season(team1_id,Year,DayNum)\n",
    "    team2 = team_regular_season(team2_id,Year,DayNum)\n",
    "    \n",
    "    # get the Ken Pom ranking for each team\n",
    "    team1Pom = get_KenPom(team1_id, Year)\n",
    "    team2Pom = get_KenPom(team2_id, Year)\n",
    "    \n",
    "    # due to the structure of the original dataframe, the first team listed is always the winner.\n",
    "    # we check if the favorite won by comparing the rankings of the two teams.\n",
    "    fav_win = pd.DataFrame(team1Pom.lt(team2Pom))\n",
    "    \n",
    "    # concatenate the dataframes and return the single game pre-game data.\n",
    "    game = pd.concat([team1.add_prefix('W_'), team2.add_prefix('L_'), fav_win], axis = 1)\n",
    "    game.rename(columns={game.columns[106]: 'fav_win'}, inplace = True)\n",
    "    \n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows how the stats from the function \"single_game\" update throughout the season. In this \n",
    "hypothetical example Duke plays North Carolina during the 2020 season on four separate occasions. On Day 0\n",
    "there are no stats available so all the values are 'Nan'. On Day 2, only Duke has played a game, they have\n",
    "stats available, however, North Carolina has not, so their values are all 'Nan'. On Day 50, both teams have\n",
    "played and have available stats. On Day 100, both teams have continued deep into their seasons and have\n",
    "updated statistics from the first 100 days of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_TeamID</th>\n",
       "      <th>W_Score</th>\n",
       "      <th>W_FGM</th>\n",
       "      <th>W_FGA</th>\n",
       "      <th>W_FGM3</th>\n",
       "      <th>W_FGA3</th>\n",
       "      <th>W_FTM</th>\n",
       "      <th>W_FTA</th>\n",
       "      <th>W_OR</th>\n",
       "      <th>W_DR</th>\n",
       "      <th>...</th>\n",
       "      <th>L_Opp_TSpct</th>\n",
       "      <th>L_Opp_FTr</th>\n",
       "      <th>L_Opp_3Pr</th>\n",
       "      <th>L_Opp_TR</th>\n",
       "      <th>L_Opp_ATOr</th>\n",
       "      <th>L_Opp_Ast_%</th>\n",
       "      <th>L_Opp_Stl_%</th>\n",
       "      <th>L_Opp_Blk_%</th>\n",
       "      <th>L_Opp_TO_r</th>\n",
       "      <th>fav_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_TeamID  W_Score  W_FGM  W_FGA  W_FGM3  W_FGA3  W_FTM  W_FTA  W_OR  W_DR  \\\n",
       "0       NaN      NaN    NaN    NaN     NaN     NaN    NaN    NaN   NaN   NaN   \n",
       "\n",
       "   ...  L_Opp_TSpct  L_Opp_FTr  L_Opp_3Pr  L_Opp_TR  L_Opp_ATOr  L_Opp_Ast_%  \\\n",
       "0  ...          NaN        NaN        NaN       NaN         NaN          NaN   \n",
       "\n",
       "   L_Opp_Stl_%  L_Opp_Blk_%  L_Opp_TO_r  fav_win  \n",
       "0          NaN          NaN         NaN     True  \n",
       "\n",
       "[1 rows x 107 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duke_unc = single_game(1181, 1314, 2020, 0)\n",
    "duke_unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_TeamID</th>\n",
       "      <th>W_Score</th>\n",
       "      <th>W_FGM</th>\n",
       "      <th>W_FGA</th>\n",
       "      <th>W_FGM3</th>\n",
       "      <th>W_FGA3</th>\n",
       "      <th>W_FTM</th>\n",
       "      <th>W_FTA</th>\n",
       "      <th>W_OR</th>\n",
       "      <th>W_DR</th>\n",
       "      <th>...</th>\n",
       "      <th>L_Opp_TSpct</th>\n",
       "      <th>L_Opp_FTr</th>\n",
       "      <th>L_Opp_3Pr</th>\n",
       "      <th>L_Opp_TR</th>\n",
       "      <th>L_Opp_ATOr</th>\n",
       "      <th>L_Opp_Ast_%</th>\n",
       "      <th>L_Opp_Stl_%</th>\n",
       "      <th>L_Opp_Blk_%</th>\n",
       "      <th>L_Opp_TO_r</th>\n",
       "      <th>fav_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181</td>\n",
       "      <td>68</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_TeamID  W_Score  W_FGM  W_FGA  W_FGM3  W_FGA3  W_FTM  W_FTA  W_OR  W_DR  \\\n",
       "0      1181       68     23     64       8      24     14     23    11    19   \n",
       "\n",
       "   ...  L_Opp_TSpct  L_Opp_FTr  L_Opp_3Pr  L_Opp_TR  L_Opp_ATOr  L_Opp_Ast_%  \\\n",
       "0  ...          NaN        NaN        NaN       NaN         NaN          NaN   \n",
       "\n",
       "   L_Opp_Stl_%  L_Opp_Blk_%  L_Opp_TO_r  fav_win  \n",
       "0          NaN          NaN         NaN     True  \n",
       "\n",
       "[1 rows x 107 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duke_unc = single_game(1181, 1314, 2020, 2)\n",
    "duke_unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_TeamID</th>\n",
       "      <th>W_Score</th>\n",
       "      <th>W_FGM</th>\n",
       "      <th>W_FGA</th>\n",
       "      <th>W_FGM3</th>\n",
       "      <th>W_FGA3</th>\n",
       "      <th>W_FTM</th>\n",
       "      <th>W_FTA</th>\n",
       "      <th>W_OR</th>\n",
       "      <th>W_DR</th>\n",
       "      <th>...</th>\n",
       "      <th>L_Opp_TSpct</th>\n",
       "      <th>L_Opp_FTr</th>\n",
       "      <th>L_Opp_3Pr</th>\n",
       "      <th>L_Opp_TR</th>\n",
       "      <th>L_Opp_ATOr</th>\n",
       "      <th>L_Opp_Ast_%</th>\n",
       "      <th>L_Opp_Stl_%</th>\n",
       "      <th>L_Opp_Blk_%</th>\n",
       "      <th>L_Opp_TO_r</th>\n",
       "      <th>fav_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181</td>\n",
       "      <td>83.857143</td>\n",
       "      <td>29.714286</td>\n",
       "      <td>66.285714</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.428571</td>\n",
       "      <td>26.285714</td>\n",
       "      <td>16.428571</td>\n",
       "      <td>27.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465617</td>\n",
       "      <td>0.184896</td>\n",
       "      <td>0.40936</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.078417</td>\n",
       "      <td>0.03866</td>\n",
       "      <td>0.143524</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_TeamID    W_Score      W_FGM      W_FGA  W_FGM3  W_FGA3      W_FTM  \\\n",
       "0      1181  83.857143  29.714286  66.285714     7.0    21.0  17.428571   \n",
       "\n",
       "       W_FTA       W_OR       W_DR  ...  L_Opp_TSpct  L_Opp_FTr  L_Opp_3Pr  \\\n",
       "0  26.285714  16.428571  27.857143  ...     0.465617   0.184896    0.40936   \n",
       "\n",
       "   L_Opp_TR  L_Opp_ATOr  L_Opp_Ast_%  L_Opp_Stl_%  L_Opp_Blk_%  L_Opp_TO_r  \\\n",
       "0      32.0    1.085714     0.527778     0.078417      0.03866    0.143524   \n",
       "\n",
       "   fav_win  \n",
       "0     True  \n",
       "\n",
       "[1 rows x 107 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duke_unc = single_game(1181, 1314, 2020, 25)\n",
    "duke_unc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W_TeamID</th>\n",
       "      <th>W_Score</th>\n",
       "      <th>W_FGM</th>\n",
       "      <th>W_FGA</th>\n",
       "      <th>W_FGM3</th>\n",
       "      <th>W_FGA3</th>\n",
       "      <th>W_FTM</th>\n",
       "      <th>W_FTA</th>\n",
       "      <th>W_OR</th>\n",
       "      <th>W_DR</th>\n",
       "      <th>...</th>\n",
       "      <th>L_Opp_TSpct</th>\n",
       "      <th>L_Opp_FTr</th>\n",
       "      <th>L_Opp_3Pr</th>\n",
       "      <th>L_Opp_TR</th>\n",
       "      <th>L_Opp_ATOr</th>\n",
       "      <th>L_Opp_Ast_%</th>\n",
       "      <th>L_Opp_Stl_%</th>\n",
       "      <th>L_Opp_Blk_%</th>\n",
       "      <th>L_Opp_TO_r</th>\n",
       "      <th>fav_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181</td>\n",
       "      <td>82.583333</td>\n",
       "      <td>30.125</td>\n",
       "      <td>62.791667</td>\n",
       "      <td>6.958333</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>15.375</td>\n",
       "      <td>21.958333</td>\n",
       "      <td>12.875</td>\n",
       "      <td>26.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517736</td>\n",
       "      <td>0.287568</td>\n",
       "      <td>0.358779</td>\n",
       "      <td>34.916667</td>\n",
       "      <td>1.122807</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.082791</td>\n",
       "      <td>0.063984</td>\n",
       "      <td>0.146231</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   W_TeamID    W_Score   W_FGM      W_FGA    W_FGM3     W_FGA3   W_FTM  \\\n",
       "0      1181  82.583333  30.125  62.791667  6.958333  19.833333  15.375   \n",
       "\n",
       "       W_FTA    W_OR       W_DR  ...  L_Opp_TSpct  L_Opp_FTr  L_Opp_3Pr  \\\n",
       "0  21.958333  12.875  26.958333  ...     0.517736   0.287568   0.358779   \n",
       "\n",
       "    L_Opp_TR  L_Opp_ATOr  L_Opp_Ast_%  L_Opp_Stl_%  L_Opp_Blk_%  L_Opp_TO_r  \\\n",
       "0  34.916667    1.122807     0.528053     0.082791     0.063984    0.146231   \n",
       "\n",
       "   fav_win  \n",
       "0     True  \n",
       "\n",
       "[1 rows x 107 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duke_unc = single_game(1181, 1314, DayNum = 100)\n",
    "duke_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can now write a function to create a full seasons worth of games using the single_game function to concatenate all of the single games of a season into one DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_season_df(df, season):\n",
    "    \n",
    "    # create a DataFrame for the season we are looking for\n",
    "    season_df = df[(df['Season'] == season) & (df['DayNum'] >= 12)]\n",
    "    season_df.reset_index(inplace = True)\n",
    "    \n",
    "    # create a list with the two teams, winning team always first, the season, and the day number of the game\n",
    "    matchups = list(zip(season_df.WTeamID, season_df.LTeamID, season_df.Season, season_df.DayNum))\n",
    "    \n",
    "    season_games = []\n",
    "    \n",
    "    # iterate through the list we created and create each single game matchup throughout the season\n",
    "    for i in range(len(matchups)):\n",
    "        game = single_game(matchups[i][0], matchups[i][1], Year = matchups[i][2], DayNum = matchups[i][3])\n",
    "        season_games.append(game)\n",
    "    \n",
    "    # from the season games list concatenate all the outputted DataFrames and insert the location\n",
    "    # of the winner (home, away, neutral)\n",
    "    df = pd.concat(season_games, axis = 0).dropna()\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    df.insert(0,\"w_loc\", season_df.WLoc)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply run this last function on each season and each postseason. With that, our data cleaning is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 658.176695 seconds\n"
     ]
    }
   ],
   "source": [
    "# ###7.5 seconds to run\n",
    "# start_time = time.time()\n",
    "\n",
    "# postseason12 = create_season_df(postseason, 2012)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print('Execution time = %.6f seconds' % (end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is very computationally expensive, because of that we saved these data frames as csv files so we never need to create these dataframes again. So We'll comment out those lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postseason13 = create_season_df(postseason, 2013)\n",
    "# postseason14 = create_season_df(postseason, 2014)\n",
    "# postseason15 = create_season_df(postseason, 2015)\n",
    "# postseason16 = create_season_df(postseason, 2016)\n",
    "# postseason17 = create_season_df(postseason, 2017)\n",
    "# postseason18 = create_season_df(postseason, 2018)\n",
    "# postseason19 = create_season_df(postseason, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 627.307349 seconds\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# regularseason12 = create_season_df(regularseason, 2012)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print('Execution time = %.6f seconds' % (end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason13 = create_season_df(regularseason, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason14 = create_season_df(regularseason, 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason15 = create_season_df(regularseason, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason16 = create_season_df(regularseason, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason17 = create_season_df(regularseason, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason18 = create_season_df(regularseason, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason19 = create_season_df(regularseason, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason20 = create_season_df(regularseason, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postseason12.to_csv('./data/postseason12.csv')\n",
    "# postseason13.to_csv('./data/postseason13.csv')\n",
    "# postseason14.to_csv('./data/postseason14.csv')\n",
    "# postseason15.to_csv('./data/postseason15.csv')\n",
    "# postseason16.to_csv('./data/postseason16.csv')\n",
    "# postseason17.to_csv('./data/postseason17.csv')\n",
    "# postseason18.to_csv('./data/postseason18.csv')\n",
    "# postseason19.to_csv('./data/postseason19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularseason12.to_csv('./data/regularseason12.csv')\n",
    "# regularseason13.to_csv('./data/regularseason13.csv')\n",
    "# regularseason14.to_csv('./data/regularseason14.csv')\n",
    "# regularseason15.to_csv('./data/regularseason15.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a clean DataFrame, we'll start trying to make to predictions.\n",
    "\n",
    "We want to take an exhaustive approach and try whatever we can to get the most accurate predictions. We're gonna try six different machine learning algorithms. We built functions for these algorithims to increase efficiency and allow us to ultimately run more models.\n",
    "\n",
    "* Logistic Regression\n",
    "* KNN\n",
    "* Decision Trees\n",
    "* Random Forests\n",
    "* Bagging Classifier\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we created a feature selection function. This function takes in data and an estimator and then using RFECV will return the best features to use for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select(X_train,y_train,estimator,min_features,step=1):\n",
    "    estimator2=estimator()\n",
    "    selector=RFECV(estimator2,min_features_to_select=min_features,step=step)\n",
    "    selector.fit(X_train,y_train)\n",
    "    selector.ranking_\n",
    "    feature_dict=dict(zip(X_train.columns, selector.ranking_))\n",
    "    best_pred = [k for (k,v) in feature_dict.items() if v == 1]\n",
    "    \n",
    "    return best_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a logistic regression function that will use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'penalty': ['l1', 'l2' ,'elasticnet'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "    \n",
    "    # Instantiate & fit LogReg model for GridSearch\n",
    "    grid_logreg = LogisticRegression(random_state=42)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_logreg, param_grid=grid, cv=cv,\n",
    "                      scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    logreg_params = gs.best_params_\n",
    "    \n",
    "    # Use best penalty from best_params\n",
    "    logreg_penalty = logreg_params['penalty']\n",
    "    print(f'Penalty: {logreg_penalty}')\n",
    "    \n",
    "    # Use best solver from best_params\n",
    "    logreg_solver = logreg_params['solver']\n",
    "    print(f'Solver: {logreg_solver}')\n",
    "    \n",
    "    # Instantiate & fit LogReg model (don't need to do this)\n",
    "    #log = LogisticRegression(random_state=42, penalty=logreg_penalty, solver=logreg_solver)\n",
    "    #log.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "  \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precision_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'F1 Score: {f1}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc_curve(gs, X_train, y_train, name='train', ax=ax)\n",
    "    plot_roc_curve(gs, X_test, y_test, name='test', ax=ax)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a KNN function that will use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, y_train, y_test, metric='minkowski', cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'n_neighbors': [1,3,5,7,9,11,13,15,17,19,21,23,25],\n",
    "    'metric': ['minkowski', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']}\n",
    "    \n",
    "    # Instantiate & fit KNN model for GridSearch\n",
    "    grid_knn = KNeighborsClassifier()\n",
    "    #grid_knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_knn, param_grid=grid, cv=cv, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    knn_params = gs.best_params_\n",
    "    \n",
    "    # Use best # of neighbors from best_params\n",
    "    knn_neighbors = knn_params['n_neighbors']\n",
    "    print(f'Number of Neighbors: {knn_neighbors}')\n",
    "    \n",
    "    # Use best metric from best_params\n",
    "    knn_metric = knn_params['metric']\n",
    "    print(f'Metric: {knn_metric}')\n",
    "    \n",
    "    # Use best weights from best_params\n",
    "    knn_weights=knn_params['weights']\n",
    "    print(f'Weights: {knn_weights}')\n",
    "    \n",
    "    # Instantiate & fit K-Nearest Neighbors model(don't need to do this)\n",
    "    #knn = KNeighborsClassifier(n_neighbors=knn_neighbors, metric=knn_metric,\n",
    "                          #     weights=knn_weights)\n",
    "    #knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "    \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precision_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'F1 score: {f1}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc_curve(gs, X_train, y_train, name='train', ax=ax)\n",
    "    plot_roc_curve(gs, X_test, y_test, name='test', ax=ax)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Decision Tree function that will use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtree(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'max_depth': [3,10,15],\n",
    "    'min_samples_split': [2,8,10,15],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    # Instantiate & fit Decision Tree model for GridSearch\n",
    "    grid_dt = DecisionTreeClassifier()\n",
    "    grid_dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_dt, param_grid=grid, cv=cv, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    dt_params = gs.best_params_\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    dt_max_depth = dt_params['max_depth']\n",
    "    print(f'Max Depth: {dt_max_depth}')\n",
    "    \n",
    "    # Use best minimum sample split from best_params\n",
    "    dt_min_samp = dt_params['min_samples_split']\n",
    "    print(f'Min Sample Split: {dt_min_samp}')\n",
    "    \n",
    "    # Use best criterion from best_params\n",
    "    dt_criterion = dt_params['criterion']\n",
    "    print(f'criterion: {dt_criterion}')\n",
    "    \n",
    "    # Instantiate & fit Decision Tree model (don't need to do this)\n",
    "   # dtree = DecisionTreeClassifier(max_depth=dt_max_depth, criterion=dt_criterion,\n",
    "           #                        min_samples_split=dt_min_samp, random_state=42)\n",
    "    #dtree.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "    \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precision_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'F1 score: {f1}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc_curve(gs, X_train, y_train, name='train', ax=ax)\n",
    "    plot_roc_curve(gs, X_test, y_test, name='test', ax=ax)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Random Forest function that will use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'n_estimators': [75,90,100,110,115,125,150,500],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    # Instantiate & fit Random Forest model for GridSearch\n",
    "    grid_rf = RandomForestClassifier()\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_rf, param_grid=grid, cv=cv, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    rf_params = gs.best_params_\n",
    "    \n",
    "    # Use best # of trees from best_params\n",
    "    rf_n_estimators = rf_params['n_estimators']\n",
    "    print(f'Number of Trees: {rf_n_estimators}')\n",
    "    \n",
    "    # Use best criterion from best_params\n",
    "    rf_criterion = rf_params['criterion']\n",
    "    print(f'Criterion: {rf_criterion}')\n",
    "    \n",
    "    # Instantiate & fit Random Forest model(don't need to do this)\n",
    "    #rforest = RandomForestClassifier(n_estimators=rf_n_estimators, criterion=rf_criterion,\n",
    "                                   # random_state=42)\n",
    "   # rforest.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    print(f'Train Mean Accuracy: {train_score}')\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    print(f'Test Mean Accuracy: {test_score}')\n",
    "    \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precision_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'F1 score: {f1}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc_curve(gs, X_train, y_train, name='train', ax=ax)\n",
    "    plot_roc_curve(gs, X_test, y_test, name='test', ax=ax)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Bagging Classifier function that will use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagged(X_train, X_test, y_train, y_test, cv=5):\n",
    "\n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'base_estimator__max_depth': [2,5,15],\n",
    "    'base_estimator__criterion': ['gini', 'entropy'],\n",
    "    'max_samples': [1,2,3,5],\n",
    "    'max_features': [1,2,3,5],\n",
    "    'n_estimators': [10,50,100,500]}\n",
    "    \n",
    "    # Instantiate & fit Bagging Classifier model for GridSearch\n",
    "    grid_bag = BaggingClassifier(DecisionTreeClassifier(), random_state=42)\n",
    "    #grid_bag.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_bag, param_grid=grid, cv=cv, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    bag_params = gs.best_params_\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    bag_max_depth = bag_params['base_estimator__max_depth']\n",
    "    print(f'Dec Tree Max Depth: {bag_max_depth}')\n",
    "    \n",
    "    # Use best criterion from best_params\n",
    "    bag_criterion = bag_params['base_estimator__criterion']\n",
    "    print(f'Dec Tree Criterion: {bag_criterion}')\n",
    "    \n",
    "    # Use best max samples from best_params\n",
    "    bag_max_sample = bag_params['max_samples']\n",
    "    print(f'Bagging Max Samples: {bag_max_sample}')\n",
    "    \n",
    "    # Use best max features from best_params\n",
    "    bag_max_features = bag_params['max_features']\n",
    "    print(f'Bag Max Features: {bag_max_features}')\n",
    "    \n",
    "    # Use best estimators from best_params\n",
    "    bag_estimators = bag_params['n_estimators']\n",
    "    print(f'# of Base Estimators: {bag_estimators}')\n",
    "    \n",
    "    # Instantiate & fit Bagging Classifier model(don't need to do this)\n",
    "    #bagging = BaggingClassifier(DecisionTreeClassifier(max_depth=bag_max_depth,\n",
    "                  #              criterion=bag_criterion), max_samples=bag_max_sample,\n",
    "                          #      max_features=bag_max_features, n_estimators=bag_estimators,\n",
    "                        #        random_state=42)\n",
    "    #bagging.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    print(f'Train Mean Accuracy Score: {train_score}')\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    print(f'Test Mean Accuracy Score: {test_score}')\n",
    "    \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precision_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'F1 score: {f1}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc_curve(gs, X_train, y_train, name='train', ax=ax)\n",
    "    plot_roc_curve(gs, X_test, y_test, name='test', ax=ax)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a XGBoost function that will use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'learning_rate': [.01,.05,.1,.5,1],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [3],\n",
    "    'subsample': [1],\n",
    "    'n_estimators': [100,500]}\n",
    "    \n",
    "    # Instantiate & fit XGClassifier\n",
    "    xgb = XGBClassifier(verbosity=0, random_state=42)\n",
    "    #xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=xgb, param_grid=grid, cv=cv, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    xgb_params = gs.best_params_\n",
    "    \n",
    "    # Use best learning rate from best_params\n",
    "    xgb_lr = xgb_params['learning_rate']\n",
    "    print(f'XGBoost Learning Rate: {xgb_lr}')\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    xgb_max_depth = xgb_params['max_depth']\n",
    "    print(f'XGBoost Max Depth: {xgb_max_depth}')\n",
    "    \n",
    "    # Use best min child weight from best_params\n",
    "    xgb_min_child_weight = xgb_params['min_child_weight']\n",
    "    print(f'XGBoost Min Child Weight: {xgb_min_child_weight}')\n",
    "    \n",
    "    # Use best subsample from best_params\n",
    "    xgb_subsample = xgb_params['subsample']\n",
    "    print(f'XGBoost Subsample: {xgb_subsample}')\n",
    "    \n",
    "    # Use best estimators from best_params\n",
    "    xgb_estimators = xgb_params['n_estimators']\n",
    "    print(f'XGBoost Estimators: {xgb_estimators}')\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = gs.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(gs, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    print(f'Train Mean Accuracy Score: {train_score}')\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    print(f'Test Mean Accuracy Score: {test_score}')\n",
    "    \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precision_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f'F1 score: {f1}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_roc_curve(gs, X_train, y_train, name='train', ax=ax)\n",
    "    plot_roc_curve(gs, X_test, y_test, name='test', ax=ax)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(gs, X_train, y_train)\n",
    "    plot_confusion_matrix(gs, X_test, y_test);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load whichever dataset you would like to train/test with, in this case there is data from the regular season of 2017 uploaded, split into an X and y and then split again into train and testing groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/regularseason17.csv').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "X = df.drop('fav_win',axis=1)\n",
    "y = df['fav_win']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline if we would have predicted only favorites to win every game\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate StandardScaler and fit on training data, then transform train and test data\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use our feature selection function to obtain the best features to use on this logistic regression model.\n",
    "log_feat=feature_select(X_train_scaled,y_train,LogisticRegression,min_features=10)\n",
    "X_train_best_log = X_train_scaled[log_feat]\n",
    "X_test_best_log = X_test_scaled[log_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our logreg function on data\n",
    "logreg(X_train_best_log, X_test_best_log, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our knn function on data\n",
    "knn(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use feature selection function to obtain best feature to use on this decision tree model\n",
    "dtree_feats=feature_select(X_train_scaled,y_train,DecisionTreeClassifier,min_features=10)\n",
    "X_train_best_dtree=X_train[dtree_feats]\n",
    "X_test_best_dtree=X_test[dtree_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our decision tree function on data\n",
    "dtree(X_train_best_dtree, X_test_best_dtree, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use feature selection function to obtain best feature to use on this random forest model\n",
    "random_forest_feat=feature_select(X_train_scaled,y_train,RandomForestClassifier,min_features=10)\n",
    "X_train_best_rforest=X_train_scaled[random_forest_feat]\n",
    "X_test_best_rforest=X_test_scaled[random_forest_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our random forest function on the data\n",
    "random_forest(X_train, X_test, y_train, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our bagging classifier function on data\n",
    "bagged(X_train, X_test, y_train, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call our XGBoost function on data\n",
    "xgboost(X_train, X_test, y_train, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing mean accuracy score and mean standard deviation across top 3 performing models on regular season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Log_Reg: 0.79088\n",
      "Std Log_Reg: 0.02652\n",
      "\n",
      "Mean Random Forest: 0.76062\n",
      "Std Random Forest: 0.04153\n",
      "\n",
      "Mean XGBoost: 0.8016\n",
      "Std XGBoost: 0.04807\n"
     ]
    }
   ],
   "source": [
    "logreg_season = [.8201, .7910, .7612, .8209, .7612]\n",
    "mean_logreg_season = round(np.mean(logreg_season), 5)\n",
    "std_logreg_season = round(np.std(logreg_season), 5)\n",
    "\n",
    "rf_season = [.73, .73, .7313, .8358, .776]\n",
    "mean_rf_season = round(np.mean(rf_season), 5)\n",
    "std_rf_season = round(np.std(rf_season), 5)\n",
    "\n",
    "xgb_season = [.86, .79, .7313, .8507, .776]\n",
    "mean_xgb_season = round(np.mean(xgb_season), 5)\n",
    "std_xgb_season = round(np.std(xgb_season), 5)\n",
    "\n",
    "print(f\"Mean Log_Reg: {mean_logreg_season}\\nStd Log_Reg: {std_logreg_season}\")\n",
    "print(f\"\\nMean Random Forest: {mean_rf_season}\\nStd Random Forest: \\\n",
    "{std_rf_season}\")\n",
    "print(f\"\\nMean XGBoost: {mean_xgb_season}\\nStd XGBoost: {std_xgb_season}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing mean accuracy score and mean standard deviation across top 3 performing models on postseason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logreg_post: 0.8\n",
      "Std Log_Reg: 0.04147\n",
      "\n",
      "Mean rf_post: 0.784\n",
      "Std Random Forest: 0.02498\n",
      "\n",
      "xgb_post: 0.764\n",
      "Std XGBoost: 0.02154\n"
     ]
    }
   ],
   "source": [
    "logreg_post = [.84, .72, .81, .82, .81]\n",
    "mean_logreg_post = round(np.mean(logreg_post), 5)\n",
    "std_logreg_post = round(np.std(logreg_post), 5)\n",
    "\n",
    "rf_post = [.81, .75, .79, .76, .81]\n",
    "mean_rf_post = round(np.mean(rf_post), 5)\n",
    "std_rf_post = round(np.std(rf_post), 5)\n",
    "\n",
    "xgb_post = [.79, .78, .77, .73, .75]\n",
    "mean_xgb_post = round(np.mean(xgb_post), 5)\n",
    "std_xgb_post = round(np.std(xgb_post), 5)\n",
    "\n",
    "print(f\"Mean logreg_post: {mean_logreg_post}\\nStd Log_Reg: {std_logreg_post}\")\n",
    "print(f\"\\nMean rf_post: {mean_rf_post}\\nStd Random Forest: \\\n",
    "{std_rf_post}\")\n",
    "print(f\"\\nxgb_post: {mean_xgb_post}\\nStd XGBoost: {std_xgb_post}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our logistic regression model in the tournament are very strong:\n",
    "\n",
    "- Overall 78% mean accuracy score for single-game predictions\n",
    "- 71% mean accuracy score for underdog predictions\n",
    "\n",
    "As such, we recommend following the model's underdog predictions for the duration of the tournament. Doing so will help maximize returns.\n",
    "\n",
    "For next steps, we'd like to explore the following:\n",
    "\n",
    "- Integrate moneyline data to further identify value\n",
    "- Incorporate more player-specific data to predict how a player will perform on a given day\n",
    "- Look at adjusting bet sizing to implement risk-adjusted wagers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
