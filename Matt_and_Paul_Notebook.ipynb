{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matt and Paul's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages and Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, accuracy_score,recall_score,precision_score,\\\n",
    "                            f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#Load Iris Data to test functions\n",
    "iris = load_iris() \n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 2), (30, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# petal length and width features\n",
    "feature_used = iris.feature_names[2:]\n",
    "X = iris.data[:, 2:] \n",
    "y = iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Builds for Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "def SS(X_train,X_test):\n",
    "    ss = StandardScaler()\n",
    "    return ss.fit_transform(X_train), ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "def logreg(X_train,X_test,y_train,y_test,cv=5):\n",
    "    grid = {\n",
    "    'penalty': ['l1','l2','elasticnet'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "    grid_logreg=LogisticRegression(random_state=42)\n",
    "    grid_logreg.fit(X_train,y_train)\n",
    "    gs = GridSearchCV(estimator=grid_logreg, param_grid=grid, cv=5, scoring='neg_log_loss')\n",
    "    gs.fit(X_train, y_train)\n",
    "    logreg_params=gs.best_params_\n",
    "    logreg_penalty=logreg_params['penalty']\n",
    "    print(f'Penalty: {logreg_penalty}')\n",
    "    logreg_solver=logreg_params['solver']\n",
    "    print(f'Solver: {logreg_solver}')\n",
    "    #Instantiate logistic regression\n",
    "    log=LogisticRegression(random_state=42,penalty=logreg_penalty,solver=logreg_solver)\n",
    "    #Fit it on train data\n",
    "    log.fit(X_train,y_train)\n",
    "    #Create y_pred using test data\n",
    "    y_pred=log.predict(X_test)\n",
    "    \n",
    "    #Use cross_val_score with cv folds\n",
    "    cv_results = cross_val_score(log, X_train, y_train, cv=cv)\n",
    "    print(f'Cross val mean score: {cv_results.mean()}')\n",
    "    \n",
    "    #Examine accuracy,recall,precision and f1 scores\n",
    "    acc_score=accuracy_score(y_test,y_pred)\n",
    "    print(f'accuracy score: {acc_score}')\n",
    "    #rec_score=recall_score(y_test,y_pred)\n",
    "    #print(f'recall score: {rec_score}')\n",
    "    #prec_score=precision_score(y_test,y_pred)\n",
    "   # print(f'precision score: {prec_score}')\n",
    "   # f1_score=f1_score(y_test,y_pred)\n",
    "   # print(f'f1 score: {f1_score}')\n",
    "    \n",
    "    #Plot an roc curve, only works with binary data\n",
    "    #plot_roc_curve(log, X_train, y_train);\n",
    "    \n",
    "    #Plot and examine confusion matrix\n",
    "    plot_confusion_matrix(log, X_train, y_train);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index in range(1,33,2):\n",
    "    #knn_model = KNeighborsClassifier(n_neighbors=index)\n",
    "    #knn_log_loss = -1 * cross_val_score(knn_model, X_train,\n",
    "                                       # y_train, scoring=\"neg_log_loss\").mean()\n",
    "    #print(f\"# of Neighbors: {index}, Log Loss Score: {knn_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def low_log_loss():\n",
    "#knn_dict={}\n",
    "#for index in range(1,33,2):\n",
    "       # knn_model = KNeighborsClassifier(n_neighbors=index)\n",
    "        #knn_log_loss = -1 * cross_val_score(knn_model, X_train,\n",
    "                                    #    y_train, scoring=\"neg_log_loss\").mean()\n",
    "        #knn_dict[index]=(knn_log_loss)\n",
    "#min_knn= min(knn_dict.values())\n",
    "#low_key = list(knn_dict.keys())[list(knn_dict.values()).index(min_knn)]\n",
    "#print(low_key)\n",
    "#print(min_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Gridsearch outside function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'n_neighbors': [1,3,5,7,9,11,13,15,17,19,21,23,25],\n",
    "    'metric': ['minkowski', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_knn=KNeighborsClassifier()\n",
    "grid_knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=grid_knn, param_grid=grid, cv=5,scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_params=gs.best_params_\n",
    "KNN_neighbors=KNN_params['n_neighbors']\n",
    "KNN_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #def low_log_loss():\n",
    "   # knn_dict={}\n",
    "   # for index in range(1,33,2):\n",
    "       # knn_model = KNeighborsClassifier(n_neighbors=index)\n",
    "       # knn_log_loss = -1 * cross_val_score(knn_model, X_train,\n",
    "                                       # y_train, scoring=\"neg_log_loss\").mean()\n",
    "       # knn_dict[index]=(knn_log_loss)\n",
    "   # min_knn= min(knn_dict.values())\n",
    "    #low_key = list(knn_dict.keys())[list(knn_dict.values()).index(min_knn)]\n",
    "   # knn=KNeighborsClassifier(n_neighbors=low_key,metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K nearest neighbors\n",
    "def KNN(X_train,X_test,y_train,y_test,metric='minkowski',cv=5):\n",
    "    grid = {\n",
    "    'n_neighbors': [1,3,5,7,9,11,13,15,17,19,21,23,25],\n",
    "    'metric': ['minkowski', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']}\n",
    "    grid_knn=KNeighborsClassifier()\n",
    "    grid_knn.fit(X_train,y_train)\n",
    "    gs = GridSearchCV(estimator=grid_knn, param_grid=grid, cv=5, scoring='neg_log_loss')\n",
    "    gs.fit(X_train, y_train)\n",
    "    KNN_params=gs.best_params_\n",
    "    KNN_neighbors=KNN_params['n_neighbors']\n",
    "    print(f'Number of Neighbors: {KNN_neighbors}')\n",
    "    KNN_metric=KNN_params['metric']\n",
    "    print(f'Metric: {KNN_metric}')\n",
    "    KNN_weights=KNN_params['weights']\n",
    "    print(f'Weights: {KNN_weights}')\n",
    "    \n",
    "    #Instantiate K nearest neighbors\n",
    "    knn=KNeighborsClassifier(n_neighbors=KNN_neighbors,metric=KNN_metric,weights=KNN_weights)\n",
    "    knn.fit(X_train,y_train)\n",
    "    #Create y_pred using test data\n",
    "    y_pred=knn.predict(X_test)\n",
    "    #Use cross_val_score with cv folds\n",
    "    cv_results = cross_val_score(knn, X_train, y_train, cv=cv)\n",
    "    print(f'Cross val mean score: {cv_results.mean()}')\n",
    "    \n",
    "    #Examine accuracy,recall,precision and f1 scores\n",
    "    acc_score=accuracy_score(y_test,y_pred)\n",
    "    print(f'accuracy score: {acc_score}')\n",
    "    #rec_score=recall_score(y_test,y_pred)\n",
    "    #print(f'recall score: {rec_score}')\n",
    "    #prec_score=precison_score(y_test,y_pred)\n",
    "    #print(f'precision score: {prec_score}')\n",
    "    #f1_score=f1_score(y_test,y_pred)\n",
    "   # print(f'f1 score: {f1_score}')\n",
    "    \n",
    "    #Plot an roc curve\n",
    "    #plot_roc_curve(knn, X_train, y_train);\n",
    "    \n",
    "    \n",
    "    #Plot and examine confusion matrix\n",
    "    plot_confusion_matrix(knn, X_train, y_train);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Trees\n",
    "def dtree(X_train,X_test,y_train,y_test,cv=5):\n",
    "    grid = {\n",
    "    'max_depth': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "    'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    grid_dt=DecisionTreeClassifier()\n",
    "    grid_dt.fit(X_train,y_train)\n",
    "    gs = GridSearchCV(estimator=grid_dt, param_grid=grid, cv=5, scoring='neg_log_loss')\n",
    "    gs.fit(X_train, y_train)\n",
    "    dt_params=gs.best_params_\n",
    "    dt_max_depth=dt_params['max_depth']\n",
    "    print(f'Max Depth: {dt_max_depth}')\n",
    "    dt_min_samp=dt_params['min_samples_split']\n",
    "    print(f'Min Sample Split: {dt_min_samp}')\n",
    "    dt_criterion=dt_params['criterion']\n",
    "    print(f'criterion: {dt_criterion}')\n",
    "    #Instantiate decision tree\n",
    "    Dtree=DecisionTreeClassifier(max_depth=dt_max_depth,criterion=dt_criterion,min_samples_split=dt_min_samp,\n",
    "                                 random_state=42)\n",
    "    #Fit it on train data\n",
    "    Dtree.fit(X_train,y_train)\n",
    "    #Create y_pred using test data\n",
    "    y_pred=Dtree.predict(X_test)\n",
    "    \n",
    "    #Use cross_val_score with cv folds\n",
    "    cv_results = cross_val_score(Dtree, X_train, y_train, cv=cv)\n",
    "    print(f'Cross val mean score: {cv_results.mean()}')\n",
    "    \n",
    "    #Examine accuracy,recall,precision and f1 scores\n",
    "    acc_score=accuracy_score(y_test,y_pred)\n",
    "    print(f'accuracy score: {acc_score}')\n",
    "    #rec_score=recall_score(y_test,y_pred)\n",
    "    #print(f'recall score: {rec_score}')\n",
    "    #prec_score=precison_score(y_test,y_pred)\n",
    "    #print(f'precision score: {prec_score}')\n",
    "    #f1_score=f1_score(y_test,y_pred)\n",
    "    #print(f'f1 score: {f1_score}')\n",
    "    \n",
    "    #Plot an roc curve\n",
    "    #plot_roc_curve(Dtree, X_train, y_train);\n",
    "    \n",
    "    #Plot and examine confusion matrix\n",
    "    plot_confusion_matrix(Dtree, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list=list(range(50,150))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train,X_test,y_train,y_test,cv=5):\n",
    "    grid = {\n",
    "    'n_estimators': n_list,\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    grid_rf=RandomForestClassifier()\n",
    "    grid_rf.fit(X_train,y_train)\n",
    "    gs = GridSearchCV(estimator=grid_rf, param_grid=grid, cv=5, scoring='neg_log_loss')\n",
    "    gs.fit(X_train, y_train)\n",
    "    rf_params=gs.best_params_\n",
    "    rf_n_estimators=rf_params['n_estimators']\n",
    "    print(f'Number Estimators: {rf_n_estimators}')\n",
    "    rf_criterion=rf_params['criterion']\n",
    "    print(f'criterion: {rf_criterion}')\n",
    "    #Instantiate decision tree\n",
    "    rforest=RandomForestClassifier(n_estimators=rf_n_estimators,criterion=rf_criterion)\n",
    "    #Fit it on train data\n",
    "    rforest.fit(X_train,y_train)\n",
    "    #Create y_pred using test data\n",
    "    y_pred=rforest.predict(X_test)\n",
    "    \n",
    "    #Use cross_val_score with cv folds\n",
    "    cv_results = cross_val_score(rforest, X_train, y_train, cv=cv)\n",
    "    print(f'Cross val mean score: {cv_results.mean()}')\n",
    "    #print forest score\n",
    "    score=rforest.score(X_test,y_test)\n",
    "    print(f'Random Forest Score: {score}')\n",
    "    \n",
    "    #Examine accuracy,recall,precision and f1 scores\n",
    "    #acc_score=accuracy_score(y_test,y_pred)\n",
    "    #print(f'accuracy score: {acc_score}')\n",
    "    #rec_score=recall_score(y_test,y_pred)\n",
    "    #print(f'recall score: {rec_score}')\n",
    "    #prec_score=precison_score(y_test,y_pred)\n",
    "    #print(f'precision score: {prec_score}')\n",
    "    #f1_score=f1_score(y_test,y_pred)\n",
    "    #print(f'f1 score: {f1_score}')\n",
    "    \n",
    "    #Plot an roc curve\n",
    "    #plot_roc_curve(Dtree, X_train, y_train);\n",
    "    \n",
    "    #Plot and examine confusion matrix\n",
    "    plot_confusion_matrix(rforest, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 5 minutes and 30 seconds to run\n",
    "random_forest(X_train,X_test,y_train,y_test,cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging\n",
    "def bagged(X_train,X_test,y_train,y_test,cv=5,max_depth=3,criterion='gini',n_estimators=10):\n",
    "\n",
    "    #Instantiate decision tree\n",
    "    bagging=BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth,criterion=criterion),\n",
    "                              n_estimators=n_estimators)\n",
    "    #Fit it on train data\n",
    "    bagging.fit(X_train,y_train)\n",
    "    #Create y_pred using test data\n",
    "    y_pred=bagging.predict(X_test)\n",
    "    \n",
    "    #Use cross_val_score with cv folds\n",
    "    cv_results = cross_val_score(bagging, X_train, y_train, cv=cv)\n",
    "    print(f'Cross val mean score: {cv_results.mean()}')\n",
    "    #print forest score\n",
    "    score=bagging.score(X_test,y_test)\n",
    "    print(f'Random Forest Score: {score}')\n",
    "    \n",
    "    #Examine accuracy,recall,precision and f1 scores\n",
    "    #acc_score=accuracy_score(y_test,y_pred)\n",
    "    #print(f'accuracy score: {acc_score}')\n",
    "    #rec_score=recall_score(y_test,y_pred)\n",
    "    #print(f'recall score: {rec_score}')\n",
    "    #prec_score=precison_score(y_test,y_pred)\n",
    "    #print(f'precision score: {prec_score}')\n",
    "    #f1_score=f1_score(y_test,y_pred)\n",
    "    #print(f'f1 score: {f1_score}')\n",
    "    \n",
    "    #Plot an roc curve\n",
    "    #plot_roc_curve(Dtree, X_train, y_train);\n",
    "    \n",
    "    #Plot and examine confusion matrix\n",
    "    plot_confusion_matrix(bagging, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged(X_train,X_test,y_train,y_test,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
